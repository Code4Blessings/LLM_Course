{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This code snippet interacts with the OpenAI API to generate real-time chat completions using a specified model (`gpt-4o-mini`) based on the provided prompts. Here's a breakdown of the code and the reasoning behind each part:\n",
      "\n",
      "### Breakdown of the Code\n",
      "\n",
      "1. **API Call to OpenAI Chat Completions Service**:\n",
      "   ```python\n",
      "   stream = openai.chat.completions.create(\n",
      "       model='gpt-4o-mini',\n",
      "       messages=prompts,\n",
      "       temperature=0.7,\n",
      "       stream=True\n",
      "   )\n",
      "   ```\n",
      "   - `openai.chat.completions.create(...)`: This function call sends a request to OpenAI's API for generating chat completions.\n",
      "   - `model='gpt-4o-mini'`: Specifies which model to use for generating the response. In this case, it uses `gpt-4o-mini`, which might be a smaller or optimized version of a GPT-4 model.\n",
      "   - `messages=prompts`: Here, `prompts` is expected to be a list of messages (input to the model). It usually consists of past messages in the conversation (user and assistant) that help the model understand the context.\n",
      "   - `temperature=0.7`: This parameter controls the randomness of the model's output. A value of 0.7 suggests that the output will be relatively creative but still somewhat focused and coherent.\n",
      "   - `stream=True`: This indicates that the response will be streamed, meaning it will be sent in smaller chunks rather than waiting for the entire response to be generated. This is useful for providing real-time feedback to the user.\n",
      "\n",
      "2. **Initializing Variables**:\n",
      "   ```python\n",
      "   reply = \"\"\n",
      "   display_handle = display(Markdown(\"\"), display_id=True)\n",
      "   ```\n",
      "   - `reply = \"\"`: Initializes an empty string variable `reply` to accumulate the received chunks of text from the streaming response.\n",
      "   - `display_handle = display(Markdown(\"\"), display_id=True)`: This code initializes a display element in a Jupyter notebook (or similar environment) to show the output in markdown format, which can be updated in real-time. `display_id=True` allows the display to be updated later.\n",
      "\n",
      "3. **Processing the Streamed Response**:\n",
      "   ```python\n",
      "   for chunk in stream:\n",
      "       reply += chunk.choices[0].delta.content or ''\n",
      "       reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
      "       update_display(Markdown(reply), display_id=display_handle.display_id)\n",
      "   ```\n",
      "   - The `for chunk in stream:` loop iterates over each chunk of the streamed response.\n",
      "     - `chunk.choices[0].delta.content`: This accesses the content of the first choice in the chunk. In a streamed response, it represents a fragment of the generated text.\n",
      "     - `reply += ...`: Appends the content of the chunk to the `reply` string. If the `content` is `None`, it appends an empty string, hence the use of `or ''`.\n",
      "   - `reply.replace(\"```\",\"\").replace(\"markdown\",\"\")`: This line removes any Markdown code blocks (```) and the word \"markdown\" from the accumulated reply. This might be done to clean up the output format, especially if the model's response included these elements.\n",
      "   - `update_display(Markdown(reply), display_id=display_handle.display_id)`: This updates the display in the notebook with the formatted Markdown output of the current accumulated `reply`, allowing users to see the response real-time as it builds.\n",
      "\n",
      "### Summary\n",
      "In essence, this code continuously collects chat-completion responses from an OpenAI API in a streaming fashion, updates the reply incrementally, and displays it in a user-friendly Markdown format. The use of streaming allows the user to see part of the response even as more is being generated, creating a responsive interaction experience.\n"
     ]
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "openai = OpenAI()\n",
    "\n",
    "response = openai.chat.completions.create(model= MODEL_GPT, messages=[{\"role\":\"user\", \"content\":question}])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8017d652-3dd3-4b39-a695-75a2ee9ac726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer and pass the question variable into the messages list\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4704c2d1-d5fd-4d77-8245-ae4702d558da",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_LLAMA = 'llama3.2'\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "payload = {\n",
    "        \"model\": MODEL_LLAMA,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b013fa8-f473-4d73-98ee-dc42b6063a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff: 100% ▕██████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6: 100% ▕██████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da: 100% ▕██████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9: 100% ▕██████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051: 100% ▕██████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# Let's just make sure the model is loaded\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6ca9b19-b4ed-4198-9251-926bde09af39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This line of code is using a feature of Python called \"generators\" and the `yield from` syntax.\n",
      "\n",
      "Here's a breakdown:\n",
      "\n",
      "1. `{...}` - This creates an expression that generates values on-the-fly, rather than computing them all at once and returning them in a list or other collection.\n",
      "2. `{book.get(\"author\") for book in books if book.get(\"author\")}` - This is an iterable expression (i.e., it can be used in a `for` loop). It uses several features:\n",
      "\t* `\"book.get(\"author\")\"`: This is a function call that returns the value associated with the key `\"author\"` from each dictionary (`book`) in the collection (`books`).\n",
      "\t* `for book in books`: This iterates over the items in the `books` collection.\n",
      "\t* `if book.get(\"author\")`: This filters out any dictionaries in `books` that don't have a value associated with the key `\"author\"`.\n",
      "3. `yield from {...}` - This is the keyword used to \"forward\" values from one generator to another.\n",
      "\n",
      "So, what does it all do together?\n",
      "\n",
      "This line of code generates an iterable sequence of authors, filtered from the `books` collection. Specifically:\n",
      "\n",
      "* It iterates over each dictionary in `books`.\n",
      "* For each dictionary, it extracts the value associated with the key `\"author\"` (or skips it if that key is not present).\n",
      "* The resulting values are then yielded as a generator.\n",
      "\n",
      "In other words, this line of code creates an iterable sequence of authors from the `books` collection, while ignoring any books without an author specified.\n",
      "\n",
      "This syntax can be useful when:\n",
      "\n",
      "* You need to process a large dataset in chunks (e.g., one book at a time).\n",
      "* You want to write efficient, lazy-based code that doesn't compute everything upfront.\n",
      "* You're working with complex data structures or APIs that don't support traditional list comprehensions.\n"
     ]
    }
   ],
   "source": [
    "# Ollama's Response\n",
    "\n",
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f6f853-9816-4afc-b6e6-18aeeaba0a40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
